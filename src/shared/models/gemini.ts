import { createGoogleGenerativeAI, type GoogleGenerativeAIProviderOptions } from '@ai-sdk/google'
import type { LanguageModelV2 } from '@ai-sdk/provider'
import { generateText } from 'ai'
import type { ProviderModelInfo } from '../types'
import type { ModelDependencies } from '../types/adapters'
import { normalizeGeminiHost } from '../utils/llm_utils'
import AbstractAISDKModel, { type CallSettings } from './abstract-ai-sdk'
import { ApiError } from './errors'
import type { CallChatCompletionOptions } from './types'

const GEMINI_IMAGE_MODELS = ['gemini-2.5-flash-image', 'gemini-3-pro-image-preview']

interface Options {
  geminiAPIKey: string
  geminiAPIHost: string
  model: ProviderModelInfo
  temperature?: number
  topP?: number
  maxOutputTokens?: number
  stream?: boolean
}

export default class Gemeni extends AbstractAISDKModel {
  public name = 'Google Gemini'

  constructor(public options: Options, dependencies: ModelDependencies) {
    super(options, dependencies)
    this.injectDefaultMetadata = false
  }

  isSupportSystemMessage() {
    return ![
      'gemini-2.0-flash-exp',
      'gemini-2.0-flash-thinking-exp',
      'gemini-2.0-flash-exp-image-generation',
      'gemini-2.5-flash-image-preview',
    ].includes(this.options.model.modelId)
  }

  protected getProvider() {
    return createGoogleGenerativeAI({
      apiKey: this.options.geminiAPIKey,
      baseURL: normalizeGeminiHost(this.options.geminiAPIHost).apiHost,
    })
  }

  protected getChatModel(_options: CallChatCompletionOptions): LanguageModelV2 {
    const provider = this.getProvider()

    return provider.chat(this.options.model.modelId)
  }

  protected getCallSettings(options: CallChatCompletionOptions): CallSettings {
    const isModelSupportThinking = this.isSupportReasoning()
    let providerParams: GoogleGenerativeAIProviderOptions = {
      safetySettings: [
        { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
      ],
    }
    if (isModelSupportThinking) {
      providerParams = {
        ...(options.providerOptions?.google || {}),
        thinkingConfig: {
          ...(options.providerOptions?.google?.thinkingConfig || {}),
          includeThoughts: true,
        },
      }
    }

    const settings: CallSettings = {
      temperature: this.options.temperature,
      topP: this.options.topP,
      maxOutputTokens: this.options.maxOutputTokens,
      providerOptions: {
        google: {
          ...providerParams,
        } satisfies GoogleGenerativeAIProviderOptions,
      },
    }
    if (['gemini-3-pro-image-preview', 'gemini-2.5-flash-image'].includes(this.options.model.modelId)) {
      settings.providerOptions = {
        google: {
          ...providerParams,
          responseModalities: ['TEXT', 'IMAGE'],
        } satisfies GoogleGenerativeAIProviderOptions,
      }
    }
    return settings
  }

  public async paint(
    params: {
      prompt: string
      images?: { imageUrl: string }[]
      num: number
      aspectRatio?: string
    },
    signal?: AbortSignal,
    callback?: (picBase64: string) => void
  ): Promise<string[]> {
    if (!GEMINI_IMAGE_MODELS.includes(this.options.model.modelId)) {
      throw new ApiError('This Gemini model does not support image generation')
    }

    const provider = this.getProvider()
    const model = provider.chat(this.options.model.modelId)

    const results: string[] = []
    for (let i = 0; i < params.num; i++) {
      const providerOptions: GoogleGenerativeAIProviderOptions = {
        responseModalities: ['TEXT', 'IMAGE'],
      }
      if (params.aspectRatio && params.aspectRatio !== 'auto') {
        providerOptions.imageConfig = { aspectRatio: params.aspectRatio }
      }

      const result = await generateText({
        model,
        messages: [{ role: 'user', content: params.prompt }],
        abortSignal: signal,
        providerOptions: {
          google: providerOptions,
        },
      })

      for (const file of result.files) {
        if (file.mediaType?.startsWith('image/') && file.base64) {
          const dataUrl = `data:${file.mediaType};base64,${file.base64}`
          results.push(dataUrl)
          callback?.(dataUrl)
        }
      }
    }
    return results
  }

  async listModels(): Promise<ProviderModelInfo[]> {
    type Response = {
      models: {
        name: string
        version: string
        displayName: string
        description: string
        inputTokenLimit: number
        outputTokenLimit: number
        supportedGenerationMethods: string[]
        temperature: number
        topP: number
        topK: number
      }[]
    }
    const res = await this.dependencies.request.apiRequest({
      url: `${this.options.geminiAPIHost}/v1beta/models?key=${this.options.geminiAPIKey}`,
      method: 'GET',
      headers: {}
    })
    const json: Response = await res.json()
    if (!json.models) {
      throw new ApiError(JSON.stringify(json))
    }
    return json.models
      .filter((m) => m.supportedGenerationMethods.some((method) => method.includes('generate')))
      .filter((m) => m.name.includes('gemini'))
      .map((m) => ({
        modelId: m.name.replace('models/', ''),
        nickname: m.displayName,
        type: 'chat' as const,
        contextWindow: m.inputTokenLimit,
        maxOutput: m.outputTokenLimit,
      }))
      .sort((a, b) => a.modelId.localeCompare(b.modelId))
  }
}
